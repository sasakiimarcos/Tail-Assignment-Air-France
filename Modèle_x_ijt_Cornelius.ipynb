{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d808f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules de base\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Module relatif à Gurobi\n",
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2bf84",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e83e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple, Optional, Union\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) File locator (decoupled)\n",
    "# ----------------------------\n",
    "\n",
    "def _format_density(density: Union[int, float, str]) -> str:\n",
    "    \"\"\"\n",
    "    Convert density to the exact string used in folder/file names.\n",
    "    Examples:\n",
    "      1    -> \"1\"\n",
    "      0.9  -> \"0.9\"\n",
    "      0.95 -> \"0.95\"\n",
    "      \"4\"  -> \"4\"\n",
    "    \"\"\"\n",
    "    if isinstance(density, str):\n",
    "        s = density.strip()\n",
    "        # Normalize \"1.0\" -> \"1\" etc.\n",
    "        try:\n",
    "            f = float(s)\n",
    "            if f.is_integer():\n",
    "                return str(int(f))\n",
    "            return s.rstrip(\"0\").rstrip(\".\") if \".\" in s else s\n",
    "        except ValueError:\n",
    "            return s\n",
    "\n",
    "    if isinstance(density, (int, float)):\n",
    "        f = float(density)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))\n",
    "        # keep minimal representation (0.90 -> 0.9)\n",
    "        s = repr(f)\n",
    "        return s.rstrip(\"0\").rstrip(\".\")\n",
    "    return str(density)\n",
    "\n",
    "\n",
    "def find_dat_file(\n",
    "    base_dir: Union[str, Path],\n",
    "    density: Union[int, float, str],\n",
    "    p: int,\n",
    "    h: int,\n",
    "    test: Optional[int] = None,\n",
    "    *,\n",
    "    top_folders: Tuple[str, str] = (\"Data\", \"Data with maintenance constraints\"),\n",
    "    folder_prefix: str = \"d=\",\n",
    "    file_prefix: str = \"DataCplex_\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Search for the .dat matching density/p/h/(optional)test under:\n",
    "      base_dir/Data/d=<density>/\n",
    "      base_dir/Data with maintenance constraints/d=<density>/\n",
    "    Returns the Path if uniquely found, otherwise raises an error.\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    d_str = _format_density(density)\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "\n",
    "    # Search both top folders (works even if one doesn't exist)\n",
    "    for top in top_folders:\n",
    "        d_folder = base_dir / top / f\"{folder_prefix}{d_str}\"\n",
    "        if not d_folder.exists():\n",
    "            continue\n",
    "\n",
    "        if test is None:\n",
    "            pattern = f\"{file_prefix}density={d_str}_p={p}_h={h}_test_*.dat\"\n",
    "        else:\n",
    "            pattern = f\"{file_prefix}density={d_str}_p={p}_h={h}_test_{test}.dat\"\n",
    "\n",
    "        candidates.extend(sorted(d_folder.glob(pattern)))\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No .dat file found for density={d_str}, p={p}, h={h}, test={test} under {base_dir}\"\n",
    "        )\n",
    "\n",
    "    # If test=None we might match multiple; require uniqueness to be safe\n",
    "    if len(candidates) > 1:\n",
    "        msg = \"\\n\".join(str(c) for c in candidates[:30])\n",
    "        more = \"\" if len(candidates) <= 30 else f\"\\n... and {len(candidates)-30} more\"\n",
    "        raise FileExistsError(\n",
    "            f\"Multiple matching .dat files found for density={d_str}, p={p}, h={h}, test={test}:\\n{msg}{more}\\n\"\n",
    "            f\"Pass test=<n> to select a unique file.\"\n",
    "        )\n",
    "\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Parser (decoupled)\n",
    "# ----------------------------\n",
    "\n",
    "_DECL_RE = re.compile(r\"(?ms)^\\s*(\\w+)\\s*=\\s*(.*?)\\s*;\\s*$\")\n",
    "\n",
    "def _parse_atom(token: str) -> Union[int, float, str]:\n",
    "    token = token.strip()\n",
    "    if token == \"\":\n",
    "        return token\n",
    "\n",
    "    # Try int then float\n",
    "    try:\n",
    "        if re.fullmatch(r\"[+-]?\\d+\", token):\n",
    "            return int(token)\n",
    "        if re.fullmatch(r\"[+-]?\\d*\\.\\d+(?:[eE][+-]?\\d+)?\", token) or re.fullmatch(r\"[+-]?\\d+(?:[eE][+-]?\\d+)\", token):\n",
    "            return float(token)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Otherwise string (e.g., airport codes A,B,C)\n",
    "    return token\n",
    "\n",
    "\n",
    "def _parse_set(value: str) -> List[Union[int, float, str]]:\n",
    "    # value like \"{A,B,C,}\" or \"{0,1,2,}\"\n",
    "    inner = value.strip()[1:-1].strip()\n",
    "    if inner == \"\":\n",
    "        return []\n",
    "    parts = [p.strip() for p in inner.split(\",\")]\n",
    "    parts = [p for p in parts if p != \"\"]\n",
    "    return [_parse_atom(p) for p in parts]\n",
    "\n",
    "\n",
    "def _parse_tuples_from_angle_brackets(value: str) -> List[Tuple[Union[int, float, str], ...]]:\n",
    "    # Extract all <...> blocks, split by commas inside\n",
    "    tuples: List[Tuple[Union[int, float, str], ...]] = []\n",
    "    for body in re.findall(r\"<([^<>]*)>\", value, flags=re.S):\n",
    "        fields = [f.strip() for f in body.split(\",\")]\n",
    "        fields = [f for f in fields if f != \"\"]\n",
    "        tuples.append(tuple(_parse_atom(f) for f in fields))\n",
    "    return tuples\n",
    "\n",
    "\n",
    "def _parse_matrix(value: str) -> List[List[Union[int, float]]]:\n",
    "    \"\"\"\n",
    "    Parses something like:\n",
    "      Cost =[\n",
    "        [6804.0,6870.0,...,]\n",
    "        [4536.0,4580.0,...,]\n",
    "      ];\n",
    "    Note: rows are NOT comma-separated in your file, so we scan bracket depth.\n",
    "    \"\"\"\n",
    "    s = value.strip()\n",
    "    if not (s.startswith(\"[\") and s.endswith(\"]\")):\n",
    "        raise ValueError(\"Matrix must start with '[' and end with ']'\")\n",
    "\n",
    "    content = s[1:-1].strip()\n",
    "\n",
    "    rows: List[str] = []\n",
    "    i = 0\n",
    "    n = len(content)\n",
    "    while i < n:\n",
    "        # Find next row '['\n",
    "        while i < n and content[i].isspace():\n",
    "            i += 1\n",
    "        if i >= n:\n",
    "            break\n",
    "        if content[i] != \"[\":\n",
    "            # skip unexpected chars\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # parse one row [...]\n",
    "        depth = 0\n",
    "        start = i\n",
    "        while i < n:\n",
    "            ch = content[i]\n",
    "            if ch == \"[\":\n",
    "                depth += 1\n",
    "            elif ch == \"]\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i\n",
    "                    rows.append(content[start + 1 : end])  # inside [...]\n",
    "                    i += 1\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "    matrix: List[List[Union[int, float]]] = []\n",
    "    for row in rows:\n",
    "        nums = [x.strip() for x in row.replace(\"\\n\", \" \").split(\",\")]\n",
    "        nums = [x for x in nums if x != \"\"]\n",
    "        parsed_row: List[Union[int, float]] = []\n",
    "        for x in nums:\n",
    "            atom = _parse_atom(x)\n",
    "            if isinstance(atom, str):\n",
    "                raise ValueError(f\"Non-numeric value in matrix: {atom!r}\")\n",
    "            parsed_row.append(atom)\n",
    "        matrix.append(parsed_row)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def parse_dat_file(dat_path: Union[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses an OPL/CPLEX-like .dat into a dict:\n",
    "      - Sets {...} -> Python list\n",
    "      - Scalars -> int/float/str\n",
    "      - Tuple lists <...> -> list[tuple]\n",
    "      - Matrices [...] with row brackets -> list[list[float/int]]\n",
    "    \"\"\"\n",
    "    dat_path = Path(dat_path)\n",
    "    text = dat_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Split by semicolons safely by matching \"name = ... ;\" blocks (multiline)\n",
    "    for m in _DECL_RE.finditer(text):\n",
    "        name = m.group(1)\n",
    "        raw = m.group(2).strip()\n",
    "\n",
    "        if raw.startswith(\"{\") and raw.endswith(\"}\"):\n",
    "            # could be a set, or a tuple-set like Flight = { <...> <...> }\n",
    "            if \"<\" in raw and \">\" in raw:\n",
    "                out[name] = _parse_tuples_from_angle_brackets(raw)\n",
    "            else:\n",
    "                out[name] = _parse_set(raw)\n",
    "\n",
    "        elif raw.startswith(\"[\") and raw.endswith(\"]\"):\n",
    "            # could be matrix, or list of tuples inside brackets\n",
    "            if \"<\" in raw and \">\" in raw:\n",
    "                out[name] = _parse_tuples_from_angle_brackets(raw)\n",
    "            else:\n",
    "                out[name] = _parse_matrix(raw)\n",
    "\n",
    "        else:\n",
    "            # scalar\n",
    "            out[name] = _parse_atom(raw)\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98493fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['Airports', 'Nbflight', 'Aircrafts', 'Flight', 'Cost', 'Aircraft'])\n",
      "Nbflight: 184\n",
      "First Flight tuple: (1, 'AMS', 'MAD', 9395.0, 9650.0)\n",
      "Cost shape: 184 x 10\n",
      "Initial aircraft position tuple: (0, 'LGW')  (Aircraft, 'Airport')\n"
     ]
    }
   ],
   "source": [
    "base = Path.cwd()  # or Path(__file__).resolve().parent\n",
    "\n",
    "# Find file by params:\n",
    "dat_file = find_dat_file(base, density=0.9, p=10, h=7, test=5)\n",
    "\n",
    "data = parse_dat_file(dat_file)\n",
    "\n",
    "print(\"Keys:\", data.keys())\n",
    "print(\"Nbflight:\", data[\"Nbflight\"])\n",
    "print(\"First Flight tuple:\", data[\"Flight\"][0])\n",
    "print(\"Cost shape:\", len(data[\"Cost\"]), \"x\", len(data[\"Cost\"][0]) if data[\"Cost\"] else 0)\n",
    "print(\"Initial aircraft position tuple:\", data[\"Aircraft\"][0], \" (Aircraft, 'Airport')\")\n",
    "\n",
    "Airports   = data[\"Airports\"] # list\n",
    "Nbflight   = data[\"Nbflight\"] # int\n",
    "Aircrafts  = data[\"Aircrafts\"] # list\n",
    "Flights    = data[\"Flight\"] # list of tuples\n",
    "Cost       = data[\"Cost\"] # matrix\n",
    "InitialPositions   = data[\"Aircraft\"] # list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8125545",
   "metadata": {},
   "source": [
    "## Add virtual flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf77a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of flights with 'virtual' first flights to enable initial position\n",
    "# + create list of flights with 'virtual' end flight to ensure end of the condition\n",
    "virtual_flights_id = -1 # start counting the virtual flight IDs in negative to not interfere with the real flights' IDs\n",
    "Original_Flights = [] # keep the original flights in a list\n",
    "Original_Flights.extend(Flights)\n",
    "\n",
    "# First flights\n",
    "First_Flights = []\n",
    "for initial_position_tuple in InitialPositions:\n",
    "    First_Flights.append((virtual_flights_id, '', initial_position_tuple[1], 0, 0))\n",
    "    virtual_flights_id = virtual_flights_id + 1\n",
    "Flights.extend(First_Flights)\n",
    "    \n",
    "# End flights for each airport\n",
    "for airport in Airports:\n",
    "    Flights.append((virtual_flights_id, airport, '', 999999999, 0))\n",
    "    virtual_flights_id = virtual_flights_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ec6c8",
   "metadata": {},
   "source": [
    "## Create matrix of possible flight combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c1229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix A_ij with possible flight combinations\n",
    "# A[i][j] will check if the flight j can happen after flight i\n",
    "A = []\n",
    "for flight in Flights:\n",
    "    row = []\n",
    "    for flight_compare in Flights:\n",
    "        # check if 'flight_compare' can happen after 'flight'\n",
    "        # this is possible if the departure time of the second flight is greater than the arrival time of the first flight\n",
    "        # and if the departure airport of the second flight is equal to the arrival airport of the first flight\n",
    "        if flight_compare[3] >= flight[4] and flight_compare[1] == flight[2]:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    # Add the row to the matrix\n",
    "    A.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e7ed2",
   "metadata": {},
   "source": [
    "## Build x_ijt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e8202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2773683\n",
      "Academic license - for non-commercial use only - expires 2027-02-02\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Duplicate keys in Model.addVars()'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28556\\3082460672.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ADD VARIABLES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mFlight_IDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFlights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddVars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlight_IDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlight_IDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAircrafts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGRB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBINARY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#X = {(i, j, t) :  m.addVar(vtype = GRB.BINARY, name = f'x_{i}_{j}_{t}') for i in Flights for j in Flights for t in Aircrafts}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc\\\\gurobipy\\\\_model.pyx\u001b[0m in \u001b[0;36mgurobipy._model.Model.addVars\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Duplicate keys in Model.addVars()'"
     ]
    }
   ],
   "source": [
    "# CREATE MODEL\n",
    "m = Model(\"x_ijt\")\n",
    "\n",
    "# ADD VARIABLES\n",
    "Flight_IDs = [f[0] for f in Flights]\n",
    "X = m.addVars(Flight_IDs, Flight_IDs, Aircrafts, vtype=GRB.BINARY, name = \"x\")\n",
    "#X = {(i, j, t) :  m.addVar(vtype = GRB.BINARY, name = f'x_{i}_{j}_{t}') for i in Flights for j in Flights for t in Aircrafts}\n",
    "\n",
    "# ADD CONSTRAINTS\n",
    "\n",
    "# Only allow combinations of flights represented in matrix A\n",
    "m.addConstrs(\n",
    "    (X[i, j, t] <= A[i[0]][j[0]]\n",
    "     for i in Flight_IDs\n",
    "     for j in Flight_IDs\n",
    "     for t in Aircrafts),\n",
    "    name=\"CombinationsMatrixCondition\"\n",
    ")\n",
    "\n",
    "# Only one plane per flight\n",
    "m.addConstrs(\n",
    "    (quicksum(\n",
    "        quicksum(\n",
    "            X[(i, j, t)]\n",
    "            for i in Flight_IDs)\n",
    "        for t in Aircrafts)\n",
    "     == 1\n",
    "     for j in Flight_IDs),\n",
    "    name = \"OnePlaneperFlightCondition\")\n",
    "\n",
    "# Continuity of the flights\n",
    "m.addConstrs(\n",
    "    (quicksum(\n",
    "        X[(i, j, t)]\n",
    "        for i in Flight_IDs)\n",
    "     ==\n",
    "     quicksum(\n",
    "        X[(j, k, t)]\n",
    "        for k in Flight_IDs)\n",
    "     for j in Flight_IDs\n",
    "     for t in Aircrafts),\n",
    "    name = \"FlightsContinuityCondition\"\n",
    ")\n",
    "\n",
    "# Initial position condition\n",
    "# Each aircraft must have one flight at the beginning\n",
    "for first in First_Flights:\n",
    "    m.addConstr(\n",
    "        quicksum(\n",
    "            X[(i, j, t)]\n",
    "            for j in Flight_IDs\n",
    "                for i in Flight_IDs\n",
    "                if i is first\n",
    "                    for t in Aircrafts\n",
    "                    if t == first[0])\n",
    "        == 1,\n",
    "        name = \"InitialPositionCondition_{first[0]}\")\n",
    "\n",
    "# Add objective function\n",
    "Original_Flights_IDs = [f[0] for f in Original_Flights]\n",
    "m.setObjective(\n",
    "    quicksum(\n",
    "        quicksum(\n",
    "            quicksum(\n",
    "                X[(i, j, t)] * Cost[t[0]][j[0]]\n",
    "                for j in Original_Flights_IDs)\n",
    "            for i in Original_Flights_IDs)\n",
    "        for t in Aircrafts),\n",
    "    GRB.MINIMIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799d16a",
   "metadata": {},
   "source": [
    "## Update and store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Choix d'un paramétrage d'affichage minimaliste --\n",
    "m.params.outputflag = 0 # mode muet\n",
    "# - \n",
    "\n",
    "# -- Mise à jour du modèle  --\n",
    "m.update()\n",
    "\n",
    "# Save model into file\n",
    "m.write(\"x_ijt_model.lp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
